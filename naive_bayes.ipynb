{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('10288320_anomaly_rm.txt')\n",
    "data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = 'a, are, an, the, and, or, but, if, then, because, about, above, after, all, also, although, am, an, and, any, as, at, be, because, been, before, being, between, both, but, by, came, can, come, could, did, do, does, each, else, for, from, get, got, had, has, have, he, her, here, him, himself, his, how, i, if, in, into, is, it, its, just, like, make, many, me, might, more, most, much, must, my, never, no, nor, not, now, of, on, only, or, other, our, out, over, said, same, see, should, since, so, some, still, such, take, than, that, the, their, them, then, there, these, they, this, those, through, to, too, under, up, use, very, want, was, way, we, well, were, what, when, where, which, while, who, will, with, would, you, your'.replace(' ','').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,e = train_test_split(data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_headline = dict()\n",
    "library_short_description = dict()\n",
    "library_total = dict()\n",
    "#word_class_count_head = dict()#단어가 key 인 dict로 각 단어는 classes value들을 가지고 있다.\n",
    "#word_class_count_desc = dict()#단어가 key 인 dict로 각 단어는 classes value들을 가지고 있다.\n",
    "word_set_headline = set()\n",
    "word_set_desc = set()\n",
    "word_set_total = set()\n",
    "classes = []\n",
    "\n",
    "for i in data:\n",
    "    token = i.replace('\\n','').lower().split('\\t')\n",
    "    if token[2] not in classes:\n",
    "        classes.append(token[2])\n",
    "\n",
    "for i in classes:\n",
    "    library_headline[i] = dict()\n",
    "    library_short_description[i] = dict()\n",
    "    library_total[i] = dict()\n",
    "\n",
    "for i in t:\n",
    "    token = i.replace('\\n','').lower().split('\\t')\n",
    "    head_split = token[1].split()\n",
    "    desc_split = token[3].split()\n",
    "    total_split = (token[1]+' '+token[3]).split()\n",
    "    for k in head_split:\n",
    "        if k in stopword:\n",
    "            continue\n",
    "        word_set_headline.add(k)\n",
    "        if k not in library_headline[token[2]]:\n",
    "            library_headline[token[2]][k] = 0\n",
    "        else:\n",
    "            library_headline[token[2]][k] +=1\n",
    "            \n",
    "    for k in desc_split:\n",
    "        if k in stopword:\n",
    "            continue\n",
    "        word_set_desc.add(k)\n",
    "        if k not in library_short_description[token[2]]:\n",
    "            library_short_description[token[2]][k] = 0\n",
    "        else:\n",
    "            library_short_description[token[2]][k] +=1\n",
    "            \n",
    "    for k in total_split:\n",
    "        if k in stopword:\n",
    "            continue\n",
    "        word_set_total.add(k)\n",
    "        if k not in library_total[token[2]]:\n",
    "            library_total[token[2]][k] = 0\n",
    "        else:\n",
    "            library_total[token[2]][k] +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del library_short_description['category']\n",
    "del library_headline['category']\n",
    "del library_total['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_headline = dict(sorted(library_headline.items(), key = lambda item: len(item[1]), reverse = True))\n",
    "library_short_description = dict(sorted(library_short_description.items(), key = lambda item: len(item[1]), reverse = True))\n",
    "library_total = dict(sorted(library_total.items(), key = lambda item: len(item[1]), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_list = list(library_headline.keys())\n",
    "desc_list = list(library_short_description.keys())\n",
    "total_list = list(library_total.keys())\n",
    "\n",
    "\n",
    "for i in headline_list:\n",
    "    library_headline[i] = dict(sorted(library_headline[i].items(), key = lambda item: (item[1]), reverse = True))\n",
    "\n",
    "for i in desc_list:\n",
    "    library_short_description[i] = dict(sorted(library_short_description[i].items(), key = lambda item: (item[1]), reverse = True))\n",
    "    \n",
    "for i in total_list:\n",
    "    library_total[i] = dict(sorted(library_total[i].items(), key = lambda item: (item[1]), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(input_string, target_classes=library_headline, vocab=word_set_headline):\n",
    "    pre_processed_string = input_string.replace('\\r','').replace('\\t','').lower().split()\n",
    "    candidate_probability = dict()\n",
    "    total_vocab_size = len(vocab)\n",
    "    for i, v in target_classes.items():#각 클래스별 순회\n",
    "        specific_class_sum = sum(v.values())\n",
    "        candidate_probability[i] = 1\n",
    "\n",
    "        for k in pre_processed_string:#문장안에 단어별로\n",
    "            if k not in v:#문장에 있는 단어중 클래스 안에 존재하지 않는 워딩 처리\n",
    "                candidate_probability[i] = candidate_probability[i] * (1) / (specific_class_sum + total_vocab_size)\n",
    "            else:\n",
    "                candidate_probability[i] = candidate_probability[i] * (v[k] + 1) / (specific_class_sum + total_vocab_size)\n",
    "                \n",
    "    top1 = list(sorted(candidate_probability.items(), key = lambda item: (item[1]), reverse = True))\n",
    "    return top1, top1[0][0]\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'https://www.huffingtonpost.comhttp://health.msn.com/health-topics/sleep-disorders/6-surprising-sleep-stealers\t6 Surprising Sleep Stealers\tWELLNESS\tIf mornings find you feeling like you just drifted off a few hours before, or if you droop with weariness no matter how much\t\t2012-03-24'\n",
    "prob, prediction = naive_bayes(a.split('\\t')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.526130668907776\n"
     ]
    }
   ],
   "source": [
    "#Evaluation with test set\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "for i in e:\n",
    "    input_split = i.split('\\t')\n",
    "    prob,prediction = naive_bayes((input_split[1]+' '+input_split[3]), target_classes=library_total, vocab=word_set_total)\n",
    "    if(i.split('\\t')[2].lower() == prediction):\n",
    "        correct_count =correct_count +1\n",
    "    total_count = total_count +1    \n",
    "\n",
    "print(correct_count / total_count * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.80483705294613\n"
     ]
    }
   ],
   "source": [
    "#Evaluation with test set\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "for i in e:\n",
    "    prob,prediction = naive_bayes(i.split('\\t')[1])\n",
    "    if(i.split('\\t')[2].lower() == prediction):\n",
    "        correct_count =correct_count +1\n",
    "    total_count = total_count +1   \n",
    "print(correct_count / total_count * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.31954430852554\n"
     ]
    }
   ],
   "source": [
    "#Evaluation with test set\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "for i in e:\n",
    "    prob,prediction = naive_bayes(i.split('\\t')[3], target_classes=library_short_description, vocab=word_set_desc)\n",
    "    if(i.split('\\t')[2].lower() == prediction):\n",
    "        correct_count =correct_count +1\n",
    "    total_count = total_count +1    \n",
    "\n",
    "print(correct_count / total_count * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
