{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\naya5\\anaconda3\\envs\\py38\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('10288320_anomaly_rm.txt')\n",
    "data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = 'a, are, an, the, and, or, but, if, then, because, about, above, after, all, also, although, am, an, and, any, as, at, be, because, been, before, being, between, both, but, by, came, can, come, could, did, do, does, each, else, for, from, get, got, had, has, have, he, her, here, him, himself, his, how, i, if, in, into, is, it, its, just, like, make, many, me, might, more, most, much, must, my, never, no, nor, not, now, of, on, only, or, other, our, out, over, said, same, see, should, since, so, some, still, such, take, than, that, the, their, them, then, there, these, they, this, those, through, to, too, under, up, use, very, want, was, way, we, well, were, what, when, where, which, while, who, will, with, would, you, your'.replace(' ','').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = 'a, are, an, the, and, or, but, if, then, because, about, above, after, all, also, although, am, an, and, any, as, at, be, because, been, before, being, between, both, but, by, came, can, come, could, did, do, does, each, else, for, from, get, got, had, has, have, he, her, here, him, himself, his, how, i, if, in, into, is, it, its, just, like, make, many, me, might, more, most, much, must, my, never, no, nor, not, now, of, on, only, or, other, our, out, over, said, same, see, should, since, so, some, still, such, take, than, that, the, their, them, then, there, these, they, this, those, through, to, too, under, up, use, very, want, was, way, we, well, were, what, when, where, which, while, who, will, with, would, you, your'.replace(' ','').split(',')\n",
    "stopword.append(\"'\")\n",
    "stopword.append('\"')\n",
    "stopword.append(',')\n",
    "stopword.append('.')\n",
    "stopword.append(';')\n",
    "stopword.append(':')\n",
    "stopword.append('?')\n",
    "stopword.append('!')\n",
    "stopword.append(')')\n",
    "stopword.append('(')\n",
    "stopword.append('@')\n",
    "\n",
    "library_headline = dict()\n",
    "library_short_description = dict()\n",
    "#word_class_count_head = dict()#단어가 key 인 dict로 각 단어는 classes value들을 가지고 있다.\n",
    "#word_class_count_desc = dict()#단어가 key 인 dict로 각 단어는 classes value들을 가지고 있다.\n",
    "word_set_headline = set()\n",
    "word_set_desc = set()\n",
    "classes = []\n",
    "\n",
    "for i in data[:int(len(data)*0.7)]:\n",
    "    token = i.replace('\\n','').lower().split('\\t')\n",
    "    if token[2] not in classes:\n",
    "        classes.append(token[2])\n",
    "\n",
    "for i in classes:\n",
    "    library_headline[i] = dict()\n",
    "    library_short_description[i] = dict()\n",
    "\n",
    "for i in data[:int(len(data)*0.7)]:\n",
    "    token = i.replace('\\n','').lower().split('\\t')\n",
    "    head_split = token[1].split()\n",
    "    desc_split = token[3].split()\n",
    "\n",
    "    head_split  = [word for word in head_split if word not in stopword]\n",
    "    desc_split  = [word for word in desc_split if word not in stopword]\n",
    "    \n",
    "    #n-gram tokening\n",
    "    head_len = len(head_split)\n",
    "    for k in range(int(n_gram//2),int(head_len-n_gram//2)):\n",
    "        #print(head_split, head_split[k-int(n_gram//2):k+int(n_gram//2+1)], k-int(n_gram//2), k+int(n_gram//2+1))\n",
    "        join_words = ''.join(head_split[k-int(n_gram//2):k+int(n_gram//2+1)])\n",
    "        word_set_headline.add(join_words)\n",
    "        if join_words not in library_headline[token[2]]:\n",
    "            library_headline[token[2]][join_words] = 0\n",
    "        else:\n",
    "            library_headline[token[2]][join_words] +=1\n",
    "            \n",
    "    desc_len = len(desc_split)\n",
    "    for k in range(int(n_gram//2),int(desc_len-n_gram//2)):\n",
    "        join_words = ''.join(desc_split[k-int(n_gram//2):k+int(n_gram//2+1)])\n",
    "        word_set_desc.add(join_words)\n",
    "        if join_words not in library_short_description[token[2]]:\n",
    "            library_short_description[token[2]][join_words] = 0\n",
    "        else:\n",
    "            library_short_description[token[2]][join_words] +=1\n",
    "\n",
    "\n",
    "del library_short_description['category']\n",
    "del library_headline['category']\n",
    "\n",
    "library_headline = dict(sorted(library_headline.items(), key = lambda item: len(item[1]), reverse = True))\n",
    "library_short_description = dict(sorted(library_short_description.items(), key = lambda item: len(item[1]), reverse = True))\n",
    "\n",
    "headline_list = list(library_headline.keys())\n",
    "desc_list = list(library_short_description.keys())\n",
    "\n",
    "for i in headline_list:\n",
    "    library_headline[i] = dict(sorted(library_headline[i].items(), key = lambda item: (item[1]), reverse = True))\n",
    "\n",
    "for i in desc_list:\n",
    "    library_short_description[i] = dict(sorted(library_short_description[i].items(), key = lambda item: (item[1]), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(input_string, target_classes=library_headline, vocab=word_set_headline):\n",
    "    pre_processed_string = input_string.replace('\\r','').replace('\\t','').lower().split()\n",
    "    candidate_probability = dict()\n",
    "    total_vocab_size = len(vocab)\n",
    "    for i, v in target_classes.items():#각 클래스별 순회\n",
    "        specific_class_sum = sum(v.values())\n",
    "        candidate_probability[i] = 1\n",
    "\n",
    "        #range(int(n_gram//2),int(desc_len-n_gram//2))\n",
    "        #join_words = ''.join(head_split[k-int(n_gram//2):k+int(n_gram//2+1)])\n",
    "        #for k in pre_processed_string:#문장안에 단어별로\n",
    "        pre_process_len = len(pre_processed_string)\n",
    "        for k in range(int(n_gram//2),int(pre_process_len-n_gram//2)):#문장안에 단어별로\n",
    "            join_words = ''.join(pre_processed_string[k-int(n_gram//2):k+int(n_gram//2+1)])\n",
    "            if join_words not in v:#문장에 있는 단어중 클래스 안에 존재하지 않는 워딩 처리\n",
    "                candidate_probability[i] = candidate_probability[i] * (1) / (specific_class_sum + total_vocab_size)\n",
    "            else:\n",
    "                candidate_probability[i] = candidate_probability[i] * (v[join_words] + 1) / (specific_class_sum + total_vocab_size)\n",
    "                \n",
    "    top1 = list(sorted(candidate_probability.items(), key = lambda item: (item[1]), reverse = True))\n",
    "    return top1, top1[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-Gram Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9561739346966727\n"
     ]
    }
   ],
   "source": [
    "#Evaluation with test set\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "for i in data[int(len(data)*0.7):]:\n",
    "    prob,prediction = naive_bayes(i.split('\\t')[1])\n",
    "    if(i.split('\\t')[2].lower() == prediction):\n",
    "        correct_count =correct_count +1\n",
    "    total_count = total_count +1    \n",
    "\n",
    "print(correct_count / total_count * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9390543779375604\n"
     ]
    }
   ],
   "source": [
    "#Evaluation with test set\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "for i in data[int(len(data)*0.7):]:\n",
    "    prob,prediction = naive_bayes(i.split('\\t')[1], target_classes=library_short_description, vocab=word_set_desc)\n",
    "    if(i.split('\\t')[2].lower() == prediction):\n",
    "        correct_count =correct_count +1\n",
    "    total_count = total_count +1    \n",
    "\n",
    "print(correct_count / total_count * 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.626108880380988\n"
     ]
    }
   ],
   "source": [
    "#Evaluation with test set\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "for i in data[int(len(data)*0.7):]:\n",
    "    prob,prediction = naive_bayes(i.split('\\t')[1])\n",
    "    if(i.split('\\t')[2].lower() == prediction):\n",
    "        correct_count =correct_count +1\n",
    "    total_count = total_count +1    \n",
    "\n",
    "print(correct_count / total_count * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.572477978024715\n"
     ]
    }
   ],
   "source": [
    "#Evaluation with test set\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "for i in data[int(len(data)*0.7):]:\n",
    "    prob,prediction = naive_bayes(i.split('\\t')[1], target_classes=library_short_description, vocab=word_set_desc)\n",
    "    if(i.split('\\t')[2].lower() == prediction):\n",
    "        correct_count =correct_count +1\n",
    "    total_count = total_count +1    \n",
    "\n",
    "print(correct_count / total_count * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
